{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf706ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.datasets import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d164b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_measure(model_name: str, dataset_name: str, epochs: int = 50,\n",
    "                      batch_size: int = 256, device: str | None = None,\n",
    "                      inference_batch_size: int = 1024,\n",
    "                      output_dir: str = \"./pykeen_results\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Training model={model_name!r} on dataset={dataset_name!r} | epochs={epochs} | batch_size={batch_size} | device={device}\")\n",
    "\n",
    "    # 1) TREINO: tempo \"wall-clock\" medido externamente + pipeline() (que tamb√©m retorna train_seconds)\n",
    "    t0 = time.perf_counter()\n",
    "    result = pipeline(\n",
    "        model=model_name,\n",
    "        dataset=dataset_name,\n",
    "        epochs=epochs,  # atalho para training_kwargs['num_epochs']\n",
    "        device=device,  # aceita 'cuda' / 'cpu' / torch.device\n",
    "        training_kwargs=dict(batch_size=batch_size, use_tqdm_batch=False),  # use_tqdm_batch=False para logs mais limpos\n",
    "    )\n",
    "    t1 = time.perf_counter()\n",
    "    wallclock_train_seconds = t1 - t0\n",
    "\n",
    "    # pipeline returns PipelineResult; try extrair os tempos informados internamente\n",
    "    train_seconds_reported = getattr(result, \"train_seconds\", None)\n",
    "    evaluate_seconds_reported = getattr(result, \"evaluate_seconds\", None)\n",
    "\n",
    "    print(f\"Wall-clock total (pipeline call) = {wallclock_train_seconds:.3f} s\")\n",
    "    if train_seconds_reported is not None:\n",
    "        print(f\"PipelineResult.train_seconds = {train_seconds_reported:.3f} s\")\n",
    "    if evaluate_seconds_reported is not None:\n",
    "        print(f\"PipelineResult.evaluate_seconds = {evaluate_seconds_reported:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a534fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 958664934.\n",
      "INFO:pykeen.datasets.utils:Loading cached preprocessed dataset from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/training\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/testing\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/validation\n",
      "INFO:pykeen.pipeline.api:Using device: cpu\n",
      "WARNING:pykeen.models.unimodal.conv_e:\n",
      "The ConvE model should be trained with inverse triples.\n",
      "This can be done by defining the TriplesFactory class with the _create_inverse_triples_ parameter set to true.\n",
      "INFO:pykeen.nn.modules:Resolving None * None * None = 200.\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model='ConvE' on dataset='Nations' | epochs=50 | batch_size=256 | device=cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grkremer\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71eddc3a61bd43b48fab8f339ef70920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.training.training_loop:Dropping last (incomplete) batch each epoch (1/6 (16.67%) batches).\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27c60ee483849858f1f3da74aab91a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cpu:   0%|          | 0.00/201 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.16s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall-clock total (pipeline call) = 22.824 s\n",
      "PipelineResult.train_seconds = 22.603 s\n",
      "PipelineResult.evaluate_seconds = 0.163 s\n"
     ]
    }
   ],
   "source": [
    "train_and_measure(\n",
    "        model_name='ConvE',\n",
    "        dataset_name='Nations',\n",
    "        epochs=50,\n",
    "        batch_size=256,\n",
    "        device='cpu',\n",
    "        inference_batch_size=1024,\n",
    "        output_dir='/pykeen_results',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b9cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 270735831.\n",
      "INFO:pykeen.datasets.utils:Loading cached preprocessed dataset from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/training\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/testing\n",
      "INFO:pykeen.triples.triples_factory:Loading from file:///C:/Users/grkremer/.data/pykeen/datasets/nations/cache/47DEQpj8HBSa-_TImW-5JCeuQeRkm5NM/validation\n",
      "INFO:pykeen.pipeline.api:Using device: cpu\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding()\n",
      "c:\\Users\\grkremer\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model='TransE' on dataset='Nations' | epochs=50 | batch_size=256 | device=cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdac22168314afdb4bbc3dcab7f50fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc36549c54ee496a9678982dcfd9cbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating on cpu:   0%|          | 0.00/201 [00:00<?, ?triple/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.08s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall-clock total (pipeline call) = 6.294 s\n",
      "PipelineResult.train_seconds = 6.172 s\n",
      "PipelineResult.evaluate_seconds = 0.082 s\n"
     ]
    }
   ],
   "source": [
    "train_and_measure(\n",
    "        model_name='TransE',\n",
    "        dataset_name='Nations',\n",
    "        epochs=50,\n",
    "        batch_size=256,\n",
    "        device='cpu',\n",
    "        inference_batch_size=1024,\n",
    "        output_dir='/pykeen_results',\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
